{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HelperFunctions import copy_subfolders_into_another_folder\n",
    "\n",
    "paths_txt = r'C:\\Users\\DELL\\Graduation Project\\Codes and Scripts\\validation.txt'\n",
    "\n",
    "# Set the source and destination folder paths\n",
    "source_folder = r\"E:\\Graduation Project\\Datasets\\Brats21\"\n",
    "destination_folder = r\"E:\\Graduation Project\\Datasets\\BraTS Validation set\"\n",
    "\n",
    "copy_subfolders_into_another_folder(paths_txt, source_folder, destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23b990e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc6f2737",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.9.0 and strictly below 2.12.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import pathlib\n",
    "import imageio\n",
    "import glob\n",
    "import PIL\n",
    "import nibabel as nib\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import cv2\n",
    "# import tensorflow_docs.vis.embed as embed\n",
    "# import tensorflow_addons.layers as tfal\n",
    "# from keras.initializers import RandomNormal\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input,Conv2D,Conv2DTranspose,LeakyReLU,Activation,Concatenate\n",
    "from scipy import ndimage\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from HelperFunctions import preprocess_image_train, generate_images_GIF, predict_image, black_seq_generator\n",
    "from SynthModels import SqueezeAttention, ResNet, unet_model, Discriminator, old_squeeze, ResNet4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73677a6",
   "metadata": {},
   "source": [
    "### Reading Cropped BraTS21 examples, change the root_path to you base address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f997f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the subset of the data, e.g. 100-200, 200-300   100-200\n"
     ]
    }
   ],
   "source": [
    "set_data = input('Enter the subset of the data, e.g. 100-200, 200-300   ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a64fb09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_path = r'E:\\Graduation Project\\Datasets\\Brain {} Cropped\\{} Niftis Cropped-3D-T Brats'.format(set_data, set_data)  \n",
    "data_list = sorted(glob.glob(root_path + '/*'))                            #list of paths of the inside files\n",
    "results_path = r'E:\\Graduation Project\\Datasets\\Synthesized Brats {} BC'.format(set_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77c5b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Just for 100-200 with ResNet7-30\n",
    "'''\n",
    "\n",
    "root_path = r'E:\\Graduation Project\\Datasets\\Brain 200-300 Cropped\\200-300 Niftis Cropped-3D-T Brats'  \n",
    "data_list = sorted(glob.glob(root_path + '/*'))                            #list of paths of the inside files\n",
    "results_path = r'E:\\Graduation Project\\Datasets\\Synthesized Brats 200-300 BC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Just for Validation Set of BraTS\n",
    "'''\n",
    "\n",
    "root_path = r'E:\\Graduation Project\\Datasets\\BraTS Validation Brain Cropped'  \n",
    "data_list = sorted(glob.glob(root_path + '/*'))                            #list of paths of the inside files\n",
    "results_path = r'E:\\Graduation Project\\Datasets\\Synthesized BraTS Validation Brain Cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b13ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Just for Validation Set of BraTS No Cropped\n",
    "'''\n",
    "\n",
    "root_path = r'E:\\Graduation Project\\Datasets\\BraTS Validation set'  \n",
    "data_list = sorted(glob.glob(root_path + '/*'))                            #list of paths of the inside files\n",
    "results_path = r'E:\\Graduation Project\\Datasets\\No Crop Synthesized BraTS Validation Brain Cropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in data_list:\n",
    "    path_num = os.path.basename(path)\n",
    "    new_result_path = os.path.join(results_path, path_num)\n",
    "\n",
    "    new_T1_path = os.path.join(new_result_path, \"images\", \"GIF T1\", \"T1_GIF\")\n",
    "    new_T2_path = os.path.join(new_result_path, \"images\", \"GIF T2\", \"T2_GIF\")\n",
    "    new_Flair_path = os.path.join(new_result_path, \"images\", \"GIF Flair\", \"Flair_GIF\")\n",
    "    \n",
    "    if not os.path.exists(new_T1_path):\n",
    "        os.makedirs(new_T1_path)\n",
    "\n",
    "    if not os.path.exists(new_T2_path):\n",
    "        os.makedirs(new_T2_path)\n",
    "\n",
    "    if not os.path.exists(new_Flair_path):\n",
    "        os.makedirs(new_Flair_path)\n",
    "\n",
    "            \n",
    "    modules_list = sorted(glob.glob(path + '/*'))  \n",
    "    flair_vol_path = modules_list[0]                      #path of flair volume is always the first ('sorted')\n",
    "    t1_vol_path = modules_list[2]                         #path of T1 volume is always the third ('sorted')\n",
    "    t2_vol_path = modules_list[4]                         #path of T2 volume is always the fifth ('sorted')\n",
    "\n",
    "    flair_nifti = nib.load(flair_vol_path).get_fdata()                    #Flair volume\n",
    "    T1_nifti = nib.load(t1_vol_path).get_fdata()                          #T1 volume\n",
    "    T2_nifti = nib.load(t2_vol_path).get_fdata()                          #T2 volume\n",
    "    \n",
    "\n",
    "    for i in range(T1_nifti.shape[2]):\n",
    "        flair_nifti_slice = flair_nifti[:, :, i]\n",
    "        t1_nifti_slice = T1_nifti[:, :, i]\n",
    "        t2_nifti_slice = T2_nifti[:, :, i]\n",
    "        \n",
    "        flair_rslt_path = f\"{new_Flair_path}/{i:03d}.png\"\n",
    "        t1_rslt_path = f\"{new_T1_path}/{i:03d}.png\"\n",
    "        t2_rslt_path = f\"{new_T2_path}/{i:03d}.png\"\n",
    "\n",
    "        imageio.imwrite(flair_rslt_path, flair_nifti_slice)\n",
    "        imageio.imwrite(t1_rslt_path, t1_nifti_slice)\n",
    "        imageio.imwrite(t2_rslt_path, t2_nifti_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea173600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brats_num = input(\"please enter the brats number  \")\n",
    "# brats_num = '028'\n",
    "seq_1 = 'T1'\n",
    "seq_2 = 'FLAIR'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a61cac",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b779ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g = SqueezeAttention().model\n",
    "generator_f = SqueezeAttention().model\n",
    "\n",
    "discriminator_x = Discriminator().model\n",
    "discriminator_y = Discriminator().model\n",
    "\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(8e-5) \n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(8e-5)\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(8e-5)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(8e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4081162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator's parameters = 10,882,179\n",
      "Discriminator's parameters = 2,765,569\n"
     ]
    }
   ],
   "source": [
    "print(\"Generator's parameters = {:,}\".format(generator_g.count_params()))\n",
    "print(\"Discriminator's parameters = {:,}\".format(discriminator_x.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14156052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Generalize Path, add it to arguments\n",
    "def generate_images_GIF(img_input, model, img_true, mode, order):\n",
    "    prediction = model(img_input)\n",
    "    pred_vol = prediction[0, :, :, 0].numpy().copy()\n",
    "    error = tf.image.ssim(img_true, prediction, max_val=2)\n",
    "    img_input = np.rot90(img_input[0, :, :, 0], 3)\n",
    "    img_true = np.rot90(img_true[0, :, :, 0], 3)\n",
    "    prediction = np.rot90(prediction[0, :, :, 0], 3)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if mode == 1:        \n",
    "        display_list = [img_input, prediction, img_true]\n",
    "        title = [f'{seq_1} True', f'{seq_2} predicted', f'{seq_2} True']\n",
    "      \n",
    "    else:\n",
    "        display_list = [img_input, prediction, img_true]\n",
    "        title = [f'{seq_2} True', f'{seq_1} predicted', f'{seq_1} True']\n",
    "    \n",
    "    plots_path_T1_FLAIR = r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF'.format(brats_num, seq_1, seq_2)\n",
    "    plots_path_FLAIR_T1 = r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF'.format(brats_num, seq_2, seq_1)\n",
    "    if not os.path.exists(plots_path_T1_FLAIR):\n",
    "        os.makedirs(plots_path_T1_FLAIR)\n",
    "    if not os.path.exists(plots_path_FLAIR_T1):\n",
    "        os.makedirs(plots_path_FLAIR_T1)\n",
    "        \n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5, cmap='gray') \n",
    "        plt.axis('off')\n",
    "        if mode == 1:\n",
    "            plt.savefig(r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF\\{}.png'.format(brats_num, seq_1, seq_2, order)) \n",
    "        if mode == 2:\n",
    "            plt.savefig(r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF\\{}.png'.format(brats_num, seq_2, seq_1, order))\n",
    "    plt.show()\n",
    "    return error, pred_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13313186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: Generalize Path, add it to arguments\n",
    "def predict_image(img_input, model, img_true):\n",
    "    prediction = model(img_input)\n",
    "    pred_vol = prediction[0, :, :, 0].numpy().copy()\n",
    "    error = tf.image.ssim(img_true, prediction, max_val=2)\n",
    "    return error, pred_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3a24b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Check Point: E:\\Graduation Project\\GIFs and Models\\Squeeze Attention T1-FLAIR-UNET-30\\ckpt-30 is restored\n"
     ]
    }
   ],
   "source": [
    "# checkpoint_path = r\"E:\\Graduation Project\\GIFs and Models\\Attention UNET-1\"\n",
    "# checkpoint_path = r\"E:\\Graduation Project\\GIFs and Models\\259 CheckPoint 17-2-2023-T1-FLAIR\"\n",
    "# checkpoint_path = r\"E:\\Graduation Project\\GIFs and Models\\ResNet-7 7-3-2023 T2-Flair-71\"\n",
    "# checkpoint_path = r\"E:\\Graduation Project\\GIFs and Models\\Squeeze Attention U-Net-30\"\n",
    "# checkpoint_path = r\"E:\\Graduation Project\\Graduation Projects GANs Conversion\\Trained_Model_epoch_150\"\n",
    "checkpoint_path = r\"E:\\Graduation Project\\GIFs and Models\\Squeeze Attention T1-FLAIR-UNET-30\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     ckpt.restore(ckpt_manager.checkpoints[30-1])\n",
    "#     ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "#     print(f'Last Check Point: {ckpt_manager.latest_checkpoint} is restored')\n",
    "    print(f'Last Check Point: {ckpt_manager.latest_checkpoint} is restored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c07ef6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the input file\n",
    "# with open(\"000-100 ResNet7-T2-FLAIR-71.log\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# # Create a new list with every fourth line\n",
    "# new_lines = lines[::4]\n",
    "\n",
    "# # Open the output file and write new lines\n",
    "# with open(\"modified_log_file.log\", \"w\") as f:\n",
    "#     f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8c0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the input file\n",
    "# with open(\"000-100 ResNet7-T2-FLAIR-71.log\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# # Remove duplicated lines\n",
    "# unique_lines = list(set(lines))\n",
    "\n",
    "# # Open the output file and write unique lines\n",
    "# with open(\"unique_log_file.log\", \"w\") as f:\n",
    "#     f.writelines(unique_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72fc8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BraTS Validation on SqzAtt T1-T2 30 epochs.log', 'w') as file:\n",
    "    file.write('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bde453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a file handler and set the formatter\n",
    "file_handler = logging.FileHandler('test smth disaster BraTS Validation on SqzAtt T1-T2 30 epochs.log')\n",
    "file_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('Brats %(iteration)s - T1-T2 ssim = %(ssim_score).4f')\n",
    "file_handler.setFormatter(formatter)\n",
    "\n",
    "# Add the file handler to the logger\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26426373",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_1 = []\n",
    "vol_2 = []\n",
    "\n",
    "ssim_1_list = []\n",
    "ssim_2_list = []\n",
    "\n",
    "order_1 = 0\n",
    "order_2 = 0\n",
    "\n",
    "ssim_score_1 = 0\n",
    "ssim_score_2 = 0\n",
    "\n",
    "seq_1 = 'T1'\n",
    "seq_2 = 'FLAIR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc4b2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Graduation Project\\\\Datasets\\\\Synthesized BraTS Validation Brain Cropped'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c5c23",
   "metadata": {},
   "source": [
    "# Copy from root to result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfe6a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source_folder = '/path/to/source/folder'\n",
    "\n",
    "# results_path = r'E:\\Graduation Project\\Datasets\\Synthesized Brats 200-300 BC'\n",
    "results_data_list = sorted(glob.glob(results_path + '/*')) \n",
    "\n",
    "T1_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\SqzAtt(30 ep) - T1 Synth from FLAIR - BraTS Validation - Brain Cropped'\n",
    "# T2_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\ResNet4(259 ep) - T1 Synth from FLAIR - BraTS Validation - Brain Cropped'\n",
    "FLAIR_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\SqzAtt(30 ep) - FLAIR Synth from T1 - BraTS Validation - Brain Cropped'\n",
    "\n",
    "# Get a list of all the subfolders in the source folder\n",
    "subfolders = [f.path for f in os.scandir(root_path) if f.is_dir()]\n",
    "\n",
    "# Copy each subfolder to the two destination folders\n",
    "for folder in subfolders:\n",
    "    shutil.copytree(folder, os.path.join(T1_syn_result_vol_path, os.path.basename(folder)))\n",
    "#     shutil.copytree(folder, os.path.join(T2_syn_result_vol_path, os.path.basename(folder)))\n",
    "    shutil.copytree(folder, os.path.join(FLAIR_syn_result_vol_path, os.path.basename(folder)))\n",
    "\n",
    "T1_syn_result_vols = sorted(glob.glob(T1_syn_result_vol_path + '/*'))\n",
    "# T2_syn_result_vols = sorted(glob.glob(T2_syn_result_vol_path + '/*'))\n",
    "FLAIR_syn_result_vols = sorted(glob.glob(FLAIR_syn_result_vol_path + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caffba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = r'E:\\Graduation Project\\Datasets\\Synthesized Brats 200-300 BC'\n",
    "results_data_list = sorted(glob.glob(results_path + '/*')) \n",
    "\n",
    "# T1_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\ResNet7-71 FLAIR Synthesized Brats 000-100 Vols'\n",
    "T2_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\ResNet7-71 T2 Synthesized Brats 200-300 Vols'\n",
    "FLAIR_syn_result_vol_path = r'E:\\Graduation Project\\Datasets\\ResNet7-71 FLAIR Synthesized Brats 200-300 Vols'\n",
    "\n",
    "# T1_syn_result_vols = sorted(glob.glob(T1_syn_result_vol_path + '/*'))\n",
    "T2_syn_result_vols = sorted(glob.glob(T2_syn_result_vol_path + '/*'))\n",
    "FLAIR_syn_result_vols = sorted(glob.glob(FLAIR_syn_result_vol_path + '/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b109c3",
   "metadata": {},
   "source": [
    "# Delete T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eced285",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_root_path = 'E:/Graduation Project/Datasets/ResNet259 T1 Synthesized Brats 000-100 Vols'\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(T1_syn_result_vol_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('t1.nii.gz'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            os.remove(file_path)\n",
    "#             print(f\"{file_path} has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37212bc0",
   "metadata": {},
   "source": [
    "# Delete T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef106373",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_root_path = 'E:/Graduation Project/Datasets/ResNet7-71 T2 Synthesized Brats 200-300 Vols'\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(T2_syn_result_vol_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('t2.nii.gz'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            os.remove(file_path)\n",
    "#             print(f\"{file_path} has been deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d88397",
   "metadata": {},
   "source": [
    "# Delete FLAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72567345",
   "metadata": {},
   "outputs": [],
   "source": [
    "del_root_path = 'E:/Graduation Project/Datasets/ResNet7-71 FLAIR Synthesized Brats 200-300 Vols'\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(FLAIR_syn_result_vol_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('flair.nii.gz'):\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "            os.remove(file_path)\n",
    "#             print(f\"{file_path} has been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7aa8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data_list = results_data_list[103:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d72a500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #0 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #1 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #2 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #3 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #4 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #5 is done\n",
      "Found 129 files belonging to 1 classes.\n",
      "Found 129 files belonging to 1 classes.\n",
      "Volume #6 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #7 is done\n",
      "Found 126 files belonging to 1 classes.\n",
      "Found 126 files belonging to 1 classes.\n",
      "Volume #8 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #9 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #10 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #11 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #12 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #13 is done\n",
      "Found 125 files belonging to 1 classes.\n",
      "Found 125 files belonging to 1 classes.\n",
      "Volume #14 is done\n",
      "Found 123 files belonging to 1 classes.\n",
      "Found 123 files belonging to 1 classes.\n",
      "Volume #15 is done\n",
      "Found 132 files belonging to 1 classes.\n",
      "Found 132 files belonging to 1 classes.\n",
      "Volume #16 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #17 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #18 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #19 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #20 is done\n",
      "Found 127 files belonging to 1 classes.\n",
      "Found 127 files belonging to 1 classes.\n",
      "Volume #21 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #22 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #23 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #24 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #25 is done\n",
      "Found 145 files belonging to 1 classes.\n",
      "Found 145 files belonging to 1 classes.\n",
      "Volume #26 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #27 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #28 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #29 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #30 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #31 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #32 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #33 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #34 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #35 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #36 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #37 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #38 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #39 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #40 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #41 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #42 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #43 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #44 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #45 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #46 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #47 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #48 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #49 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #50 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #51 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #52 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #53 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #54 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #55 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #56 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #57 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #58 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #59 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #60 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #61 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #62 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #63 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #64 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #65 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #66 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #67 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #68 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #69 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #70 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #71 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #72 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #73 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #74 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #75 is done\n",
      "Found 132 files belonging to 1 classes.\n",
      "Found 132 files belonging to 1 classes.\n",
      "Volume #76 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #77 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #78 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #79 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #80 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #81 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #82 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume #83 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #84 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #85 is done\n",
      "Found 145 files belonging to 1 classes.\n",
      "Found 145 files belonging to 1 classes.\n",
      "Volume #86 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #87 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #88 is done\n",
      "Found 123 files belonging to 1 classes.\n",
      "Found 123 files belonging to 1 classes.\n",
      "Volume #89 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #90 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #91 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #92 is done\n",
      "Found 133 files belonging to 1 classes.\n",
      "Found 133 files belonging to 1 classes.\n",
      "Volume #93 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #94 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #95 is done\n",
      "Found 132 files belonging to 1 classes.\n",
      "Found 132 files belonging to 1 classes.\n",
      "Volume #96 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #97 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #98 is done\n",
      "Found 127 files belonging to 1 classes.\n",
      "Found 127 files belonging to 1 classes.\n",
      "Volume #99 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #100 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #101 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #102 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #103 is done\n",
      "Found 145 files belonging to 1 classes.\n",
      "Found 145 files belonging to 1 classes.\n",
      "Volume #104 is done\n",
      "Found 147 files belonging to 1 classes.\n",
      "Found 147 files belonging to 1 classes.\n",
      "Volume #105 is done\n",
      "Found 151 files belonging to 1 classes.\n",
      "Found 151 files belonging to 1 classes.\n",
      "Volume #106 is done\n",
      "Found 150 files belonging to 1 classes.\n",
      "Found 150 files belonging to 1 classes.\n",
      "Volume #107 is done\n",
      "Found 154 files belonging to 1 classes.\n",
      "Found 154 files belonging to 1 classes.\n",
      "Volume #108 is done\n",
      "Found 150 files belonging to 1 classes.\n",
      "Found 150 files belonging to 1 classes.\n",
      "Volume #109 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #110 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #111 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #112 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #113 is done\n",
      "Found 132 files belonging to 1 classes.\n",
      "Found 132 files belonging to 1 classes.\n",
      "Volume #114 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #115 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #116 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #117 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #118 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #119 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #120 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #121 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #122 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #123 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #124 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #125 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #126 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #127 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #128 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #129 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #130 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #131 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #132 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #133 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #134 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #135 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #136 is done\n",
      "Found 129 files belonging to 1 classes.\n",
      "Found 129 files belonging to 1 classes.\n",
      "Volume #137 is done\n",
      "Found 129 files belonging to 1 classes.\n",
      "Found 129 files belonging to 1 classes.\n",
      "Volume #138 is done\n",
      "Found 132 files belonging to 1 classes.\n",
      "Found 132 files belonging to 1 classes.\n",
      "Volume #139 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #140 is done\n",
      "Found 128 files belonging to 1 classes.\n",
      "Found 128 files belonging to 1 classes.\n",
      "Volume #141 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #142 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #143 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #144 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #145 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #146 is done\n",
      "Found 133 files belonging to 1 classes.\n",
      "Found 133 files belonging to 1 classes.\n",
      "Volume #147 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #148 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #149 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #150 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #151 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #152 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #153 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #154 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #155 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #156 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #157 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #158 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #159 is done\n",
      "Found 129 files belonging to 1 classes.\n",
      "Found 129 files belonging to 1 classes.\n",
      "Volume #160 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #161 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #162 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #163 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #164 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #165 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume #166 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #167 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #168 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #169 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #170 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #171 is done\n",
      "Found 152 files belonging to 1 classes.\n",
      "Found 152 files belonging to 1 classes.\n",
      "Volume #172 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #173 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #174 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #175 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #176 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #177 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #178 is done\n",
      "Found 133 files belonging to 1 classes.\n",
      "Found 133 files belonging to 1 classes.\n",
      "Volume #179 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #180 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #181 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #182 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #183 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #184 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #185 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #186 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #187 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #188 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #189 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #190 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #191 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #192 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #193 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #194 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #195 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #196 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #197 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #198 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #199 is done\n",
      "Found 128 files belonging to 1 classes.\n",
      "Found 128 files belonging to 1 classes.\n",
      "Volume #200 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #201 is done\n",
      "Found 144 files belonging to 1 classes.\n",
      "Found 144 files belonging to 1 classes.\n",
      "Volume #202 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #203 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #204 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #205 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #206 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #207 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #208 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #209 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #210 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #211 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #212 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #213 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #214 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #215 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #216 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #217 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #218 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #219 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #220 is done\n",
      "Found 139 files belonging to 1 classes.\n",
      "Found 139 files belonging to 1 classes.\n",
      "Volume #221 is done\n",
      "Found 133 files belonging to 1 classes.\n",
      "Found 133 files belonging to 1 classes.\n",
      "Volume #222 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #223 is done\n",
      "Found 131 files belonging to 1 classes.\n",
      "Found 131 files belonging to 1 classes.\n",
      "Volume #224 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #225 is done\n",
      "Found 145 files belonging to 1 classes.\n",
      "Found 145 files belonging to 1 classes.\n",
      "Volume #226 is done\n",
      "Found 147 files belonging to 1 classes.\n",
      "Found 147 files belonging to 1 classes.\n",
      "Volume #227 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #228 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #229 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #230 is done\n",
      "Found 149 files belonging to 1 classes.\n",
      "Found 149 files belonging to 1 classes.\n",
      "Volume #231 is done\n",
      "Found 141 files belonging to 1 classes.\n",
      "Found 141 files belonging to 1 classes.\n",
      "Volume #232 is done\n",
      "Found 136 files belonging to 1 classes.\n",
      "Found 136 files belonging to 1 classes.\n",
      "Volume #233 is done\n",
      "Found 130 files belonging to 1 classes.\n",
      "Found 130 files belonging to 1 classes.\n",
      "Volume #234 is done\n",
      "Found 137 files belonging to 1 classes.\n",
      "Found 137 files belonging to 1 classes.\n",
      "Volume #235 is done\n",
      "Found 133 files belonging to 1 classes.\n",
      "Found 133 files belonging to 1 classes.\n",
      "Volume #236 is done\n",
      "Found 138 files belonging to 1 classes.\n",
      "Found 138 files belonging to 1 classes.\n",
      "Volume #237 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #238 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #239 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #240 is done\n",
      "Found 146 files belonging to 1 classes.\n",
      "Found 146 files belonging to 1 classes.\n",
      "Volume #241 is done\n",
      "Found 149 files belonging to 1 classes.\n",
      "Found 149 files belonging to 1 classes.\n",
      "Volume #242 is done\n",
      "Found 142 files belonging to 1 classes.\n",
      "Found 142 files belonging to 1 classes.\n",
      "Volume #243 is done\n",
      "Found 145 files belonging to 1 classes.\n",
      "Found 145 files belonging to 1 classes.\n",
      "Volume #244 is done\n",
      "Found 143 files belonging to 1 classes.\n",
      "Found 143 files belonging to 1 classes.\n",
      "Volume #245 is done\n",
      "Found 140 files belonging to 1 classes.\n",
      "Found 140 files belonging to 1 classes.\n",
      "Volume #246 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n",
      "Volume #247 is done\n",
      "Found 134 files belonging to 1 classes.\n",
      "Found 134 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume #248 is done\n",
      "Found 127 files belonging to 1 classes.\n",
      "Found 127 files belonging to 1 classes.\n",
      "Volume #249 is done\n",
      "Found 135 files belonging to 1 classes.\n",
      "Found 135 files belonging to 1 classes.\n",
      "Volume #250 is done\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "ssim1 = []\n",
    "ssim2 = []\n",
    "\n",
    "for i, path in enumerate(results_data_list):\n",
    "#     if kkk == 0:\n",
    "#         kkk = 1\n",
    "#         continue\n",
    "    T1_path = os.path.join(path, \"images\", \"GIF T1\")\n",
    "#     T2_path = os.path.join(path, \"images\", \"GIF T2\")\n",
    "    FLAIR_path = os.path.join(path, \"images\", \"GIF Flair\")\n",
    "\n",
    "    dep = len(glob.glob(T1_path + '/**/*'))\n",
    "    \n",
    "    GIF_T1 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                  T1_path,\n",
    "                                  seed=123,\n",
    "                                  image_size=(256, 256),\n",
    "                                  batch_size=1,\n",
    "                                  shuffle = False)\n",
    "\n",
    "#     GIF_T2 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "#                                   T2_path,\n",
    "#                                   seed=123,\n",
    "#                                   image_size=(256, 256),\n",
    "#                                   batch_size=1,\n",
    "#                                   shuffle = False)\n",
    "    \n",
    "    GIF_FLAIR = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                  FLAIR_path,\n",
    "                                  seed=123,\n",
    "                                  image_size=(256, 256),\n",
    "                                  batch_size=1,\n",
    "                                  shuffle = False)\n",
    "                            \n",
    "\n",
    "    GIF_T1 = GIF_T1.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#     GIF_T2 = GIF_T2.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    GIF_FLAIR = GIF_FLAIR.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    GIF_T1 = GIF_T1.map(lambda x, _: (preprocess_image_train(x)))\n",
    "#     GIF_T2 = GIF_T2.map(lambda x, _: (preprocess_image_train(x)))\n",
    "    GIF_FLAIR = GIF_FLAIR.map(lambda x, _: (preprocess_image_train(x)))\n",
    "\n",
    "\n",
    "    for image_x, image_y in tf.data.Dataset.zip((GIF_T1, GIF_FLAIR)):\n",
    "        ssim_1, img_1 = predict_image(image_x, generator_g, image_y)\n",
    "        ssim_score_1 = ssim_score_1 + ssim_1\n",
    "        ssim_2_list.append(ssim_1)\n",
    "        vol_2.append(img_1)\n",
    "\n",
    "    for image_x, image_y in tf.data.Dataset.zip((GIF_T1, GIF_FLAIR)):\n",
    "        ssim_2, img_2 = predict_image(image_y, generator_f, image_x)\n",
    "        ssim_score_2 = ssim_score_2 + ssim_2\n",
    "        ssim_1_list.append(ssim_2)\n",
    "        vol_1.append(img_2)\n",
    "    \n",
    "    ssim_score_1, ssim_score_2 = ssim_score_1/dep , ssim_score_2/dep\n",
    "#     logger.info('', extra={'iteration': i+200, 'ssim_score': ssim_score_1.numpy()[0]})\n",
    "#     logger.info('', extra={'iteration': i+200, 'ssim_score': ssim_score_2.numpy()[0]})\n",
    "\n",
    "\n",
    "    original_vol_path = sorted(glob.glob(data_list[i] + '/*'))[0]\n",
    "    original_vol = nib.load(original_vol_path)\n",
    "    original_shape = original_vol.shape\n",
    "\n",
    "\n",
    "    vol_1 = np.array(vol_1).transpose(1, 2, 0)\n",
    "    vol_2 = np.array(vol_2).transpose(1, 2, 0)\n",
    "\n",
    "    vol_1 = ndimage.zoom(vol_1, (original_shape[0]/vol_1.shape[0],\n",
    "                                 original_shape[1]/vol_1.shape[1],\n",
    "                                 original_shape[2]/vol_1.shape[2]), order=0)\n",
    "\n",
    "    vol_2 = ndimage.zoom(vol_2, (original_shape[0]/vol_2.shape[0],\n",
    "                                 original_shape[1]/vol_2.shape[1],\n",
    "                                 original_shape[2]/vol_2.shape[2]), order=0)\n",
    "\n",
    "\n",
    "    v1 = nib.Nifti1Image(np.array(vol_1), original_vol.affine)            # to save this 3D (ndarry) numpy\n",
    "    v2 = nib.Nifti1Image(np.array(vol_2), original_vol.affine)            # to save this 3D (ndarry) numpy\n",
    "\n",
    "    T1_name = os.path.basename(sorted(glob.glob(data_list[i] + '/*'))[2])\n",
    "#     T2_name = os.path.basename(sorted(glob.glob(data_list[i] + '/*'))[4])\n",
    "    FLAIR_name = os.path.basename(sorted(glob.glob(data_list[i] + '/*'))[0])\n",
    "\n",
    "    T1_res_path = os.path.join(T1_syn_result_vols[i], T1_name)\n",
    "#     T2_res_path = os.path.join(T2_syn_result_vols[i], T2_name)\n",
    "    FLAIR_res_path = os.path.join(FLAIR_syn_result_vols[i], FLAIR_name)\n",
    "\n",
    "\n",
    "    # Save the Nifti image to a file\n",
    "    nib.save(v1, T1_res_path)\n",
    "#     nib.save(v2, T2_res_path)\n",
    "    nib.save(v2, FLAIR_res_path)\n",
    "\n",
    "    \n",
    "    vol_1 = []\n",
    "    vol_2 = []\n",
    "    \n",
    "    ssim1.append(ssim_score_1)\n",
    "    ssim2.append(ssim_score_2)\n",
    "\n",
    "    ssim_score_1 = 0\n",
    "    ssim_score_2 = 0\n",
    "    print(f\"Volume #{i} is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e87730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a72815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    \"From T1 to FLAIR\": np.array(ssim2)[:, 0],\n",
    "    'From FLAIR to T1': np.array(ssim1)[:, 0]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('T1-FLAIR SqzAtt-30 SSIM.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9abef04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume #0 is done\n"
     ]
    }
   ],
   "source": [
    "T2_res_path = os.path.join(T2_syn_result_vols[i], T2_name)\n",
    "FLAIR_res_path = os.path.join(FLAIR_syn_result_vols[i], FLAIR_name)\n",
    "\n",
    "\n",
    "# Save the Nifti image to a file\n",
    "nib.save(v1, FLAIR_res_path)\n",
    "#     nib.save(v2, T2_res_path)\n",
    "nib.save(v2, T2_res_path)\n",
    "\n",
    "\n",
    "vol_1 = []\n",
    "vol_2 = []\n",
    "\n",
    "ssim_score_1 = 0\n",
    "ssim_score_2 = 0\n",
    "print(f\"Volume #{i} is done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "111a4a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 256, 256)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(vol_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1cc58f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssim_score for first Conversion = 0.6095 \n",
      "ssim_score for second Conversion = 0.6937 \n"
     ]
    }
   ],
   "source": [
    "ssim_score_1, ssim_score_2 = ssim_score_1/dep , ssim_score_2/dep\n",
    "\n",
    "\n",
    "print(\"ssim_score for first Conversion = {:.4f} \".format(ssim_score_1.numpy()[0]))\n",
    "print(\"ssim_score for second Conversion = {:.4f} \".format(ssim_score_2.numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1136fb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 3724.0\n",
      "-1.0 0.9064252972602844\n"
     ]
    }
   ],
   "source": [
    "original_vol_path = sorted(glob.glob(root_path + '/*'))[0]\n",
    "original_vol = nib.load(original_vol_path)\n",
    "x = original_vol.get_fdata()\n",
    "print(x.min(), x.max())\n",
    "\n",
    "\n",
    "\n",
    "original_vol = nib.load(r'E:\\Graduation Project\\GIFs and Models\\Brats 006\\Testing Synthesis With Segmentation on BraTS #006\\FLAIR.nii.gz')\n",
    "x = original_vol.get_fdata()\n",
    "print(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8afb1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_vol_path = sorted(glob.glob(root_path + '/*'))[0]\n",
    "original_vol = nib.load(original_vol_path)\n",
    "original_shape = original_vol.shape\n",
    "\n",
    "result_vol_path = r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Synthesized Volumes'.format(brats_num)\n",
    "if not os.path.exists(result_vol_path):\n",
    "    os.makedirs(result_vol_path)\n",
    "\n",
    "vol_1 = np.array(vol_1).transpose(1, 2, 0)\n",
    "vol_2 = np.array(vol_2).transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "vol_1 = ndimage.zoom(vol_1, (original_shape[0]/vol_1.shape[0],\n",
    "                                             original_shape[1]/vol_1.shape[1],\n",
    "                                             original_shape[2]/vol_1.shape[2]), order=0)\n",
    "\n",
    "vol_2 = ndimage.zoom(vol_2, (original_shape[0]/vol_2.shape[0],\n",
    "                                             original_shape[1]/vol_2.shape[1],\n",
    "                                             original_shape[2]/vol_2.shape[2]), order=0)\n",
    "\n",
    "    \n",
    "v1 = nib.Nifti1Image(np.array(vol_1), original_vol.affine)            # to save this 3D (ndarry) numpy\n",
    "v2 = nib.Nifti1Image(np.array(vol_2), original_vol.affine)            # to save this 3D (ndarry) numpy\n",
    "\n",
    "\n",
    "# Save the Nifti image to a file\n",
    "nib.save(v1, result_vol_path + '/' + 'T1.nii.gz')\n",
    "nib.save(v2, result_vol_path + '/' + 'T2.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}.gif'.format(brats_num, seq_1, seq_2)\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob(r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF\\*.png'.format(brats_num, seq_1, seq_2))\n",
    "  filenames = sorted(filenames)\n",
    "  filenames = list(Tcl().call('lsort', '-dict', filenames))\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a375c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}.gif'.format(brats_num, seq_2, seq_1)\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob(r'E:\\Graduation Project\\GIFs and Models\\Brats {}\\Predicted\\{}-{}-GIF\\*.png'.format(brats_num, seq_2, seq_1))\n",
    "  filenames = sorted(filenames)\n",
    "  filenames = list(Tcl().call('lsort', '-dict', filenames))\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)\n",
    "\n",
    "embed.embed_file(anim_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a8ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6a519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6612cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94976cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d488c05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9900484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for i, path in enumerate(results_data_list):\n",
    "#     if kkk == 0:\n",
    "#         kkk = 1\n",
    "#         continue\n",
    "    T1_path = os.path.join(path, \"images\", \"GIF T1\")\n",
    "    T2_path = os.path.join(path, \"images\", \"GIF T2\")\n",
    "    FLAIR_path = os.path.join(path, \"images\", \"GIF Flair\")\n",
    "\n",
    "    dep = len(glob.glob(T1_path + '/**/*'))\n",
    "    \n",
    "    GIF_T1 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                  T1_path,\n",
    "                                  seed=123,\n",
    "                                  image_size=(256, 256),\n",
    "                                  batch_size=1,\n",
    "                                  shuffle = False)\n",
    "\n",
    "    GIF_T2 = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "                                  T2_path,\n",
    "                                  seed=123,\n",
    "                                  image_size=(256, 256),\n",
    "                                  batch_size=1,\n",
    "                                  shuffle = False)\n",
    "\n",
    "\n",
    "    GIF_T1 = GIF_T1.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    GIF_T2 = GIF_T2.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    GIF_T1 = GIF_T1.map(lambda x, _: (preprocess_image_train(x)))\n",
    "    GIF_T2 = GIF_T2.map(lambda x, _: (preprocess_image_train(x)))\n",
    "\n",
    "\n",
    "    for image_x, image_y in tf.data.Dataset.zip((GIF_T1, GIF_T2)):\n",
    "#         ssim_1, img_1 = predict_image_and_calc_loss(image_x, generator_f, image_y)\n",
    "#         ssim_score_1 = ssim_score_1 + ssim_1\n",
    "#         ssim_1_list.append(ssim_1)\n",
    "#         vol_1.append(img_1)\n",
    "        prediction = generator_g(image_x)\n",
    "        print(image_x.numpy().mean(), prediction.numpy().mean())\n",
    "#         print(image_x.numpy().min(), prediction.numpy().min())\n",
    "#         plt.imshow(image_y.numpy()[0])\n",
    "        k = k + 1\n",
    "        break\n",
    "    if k == 200:\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image_y.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c909beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in enumerate(results_data_list):\n",
    "    T1_path = os.path.join(path, \"images\", \"GIF T1\")\n",
    "    T2_path = os.path.join(path, \"images\", \"GIF T2\")\n",
    "    FLAIR_path = os.path.join(path, \"images\", \"GIF Flair\")\n",
    "\n",
    "    print(len(glob.glob(T1_path + '/**/*')))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28151efd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
