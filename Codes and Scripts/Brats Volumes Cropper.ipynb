{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6de87aea",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "551b1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73677a6",
   "metadata": {},
   "source": [
    "### Reading all BraTS21 examples, change the root_path to you base address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9a64fb09",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root_path = r'C:\\Users\\DELL\\Graduation Project\\Datasets\\Brats21'                     #Change this to dataset base address\n",
    "data_list = sorted(glob.glob(root_path + '/*'))                                      #list of paths of the inside files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4db90",
   "metadata": {},
   "source": [
    "#### The following function is responsible for returning the indices of the brain of the volume that contains foreground voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cba4849f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def find_brain_width_wise(dep, hei, i, img):        #cropping width wise\n",
    "    slice2D = img.get_fdata()[:, i, :]\n",
    "    for j in range(hei):\n",
    "        for k in range(dep):\n",
    "            if slice2D[j, k] != 0:\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "def find_brain_height_wise(dep, wid, i, img):      #cropping height wise\n",
    "    slice2D = img.get_fdata()[i, :, :]\n",
    "    for j in range(wid):\n",
    "        for k in range(dep):\n",
    "            if slice2D[j, k] != 0:\n",
    "                return i\n",
    "    return 0\n",
    "\n",
    "def find_brain_depth_wise(wid, hei, i, img):        #cropping depth wise\n",
    "    slice2D = img.get_fdata()[:, :, i]\n",
    "    for j in range(wid):\n",
    "        for k in range(hei):\n",
    "            if slice2D[j, k] != 0:\n",
    "                return i\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3178c0c",
   "metadata": {},
   "source": [
    "#### Creating results folder that will contain the cropped volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1438f3e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create results folder\n",
    "results_path = r'C:\\Users\\DELL\\Graduation Project\\Datasets\\xx'\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a60df6cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\DELL\\\\Graduation Project\\\\Datasets\\\\Brats21\\\\BraTS2021_00000\\\\BraTS2021_00000_flair.nii.gz',\n",
       " 'C:\\\\Users\\\\DELL\\\\Graduation Project\\\\Datasets\\\\Brats21\\\\BraTS2021_00000\\\\BraTS2021_00000_seg.nii.gz',\n",
       " 'C:\\\\Users\\\\DELL\\\\Graduation Project\\\\Datasets\\\\Brats21\\\\BraTS2021_00000\\\\BraTS2021_00000_t1.nii.gz',\n",
       " 'C:\\\\Users\\\\DELL\\\\Graduation Project\\\\Datasets\\\\Brats21\\\\BraTS2021_00000\\\\BraTS2021_00000_t1ce.nii.gz',\n",
       " 'C:\\\\Users\\\\DELL\\\\Graduation Project\\\\Datasets\\\\Brats21\\\\BraTS2021_00000\\\\BraTS2021_00000_t2.nii.gz']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(glob.glob(data_list[0] + '/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b5f75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In case of the depth cropping\n",
      "Do you want it to be cropped based on the brain voxels or the tumor voxels?\n",
      "please enter either 'brain', or 'tumor' \n",
      "tumor\n"
     ]
    }
   ],
   "source": [
    "depth_cropper_type = input(\"In case of the depth cropping\\n\"\n",
    "                           \"Do you want it to be cropped based on the brain voxels or the tumor voxels?\\n\"\n",
    "                           \"please enter either 'brain', or 'tumor' \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1ef2b2",
   "metadata": {},
   "source": [
    "### Looping in every example and in every module in the volume to crop it to contain only the brain part without the background slices. In addition, we will also crop it depthwise to contain the tumorous slices only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c03b6116",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Iterating for every example in the folder\n",
    "for example in range(0, 1):                           # The number of example I want to crop and save from the data\n",
    "    modules_list = sorted(glob.glob(data_list[example] + '/*'))          #modules list of a single volume\n",
    "    vol_path = modules_list[0]                        #It doesn't matter which mode, just no seg (tumor), as we want the brain\n",
    "    img = nib.load(vol_path)                          #Flair volume (it doesn't matter)\n",
    "    \n",
    "    seg_vol_path = modules_list[1]                    #Segmentation volume (second volume)\n",
    "    img_tumor = nib.load(seg_vol_path)\n",
    "\n",
    "    height, width, depth = img.shape\n",
    "    \n",
    "    filled_slices_width = []                              #slices indices with foreground values\n",
    "    filled_slices_height = []\n",
    "    filled_slices_depth = []\n",
    "\n",
    "    #Iterating in the dimension of interest, which we will extract foreground slices from, repeating it in the 3 dimension (3D Volume)\n",
    "\n",
    "    if (depth_cropper_type == 'brain'):\n",
    "        depth_img = img\n",
    "    elif (depth_cropper_type == 'tumor'):\n",
    "        depth_img = img_tumor\n",
    "    else:\n",
    "        print(\"No cropping has been made.\")\n",
    "        break\n",
    "    \n",
    "    for i in range(width):\n",
    "        width_idx = find_brain_width_wise(depth, height, i, img)       #img not img_tumor in the width and height\n",
    "        if width_idx != 0:\n",
    "            filled_slices_width.append(width_idx)\n",
    "\n",
    "    for i in range(height):\n",
    "        height_idx = find_brain_height_wise(depth, width, i, img)\n",
    "        if height_idx != 0:\n",
    "            filled_slices_height.append(height_idx)\n",
    "        \n",
    "\n",
    "    for i in range(depth):           \n",
    "        depth_idx = find_brain_depth_wise(width, height, i, depth_img)     #depth_img is either img or img_tumor\n",
    "        if depth_idx != 0:\n",
    "            filled_slices_depth.append(depth_idx)\n",
    "\n",
    "    min_wid_idx, max_wid_idx = filled_slices_width[0], filled_slices_width[-1]\n",
    "    min_hei_idx, max_hei_idx = filled_slices_height[0], filled_slices_height[-1]\n",
    "    min_dep_idx, max_dep_idx = filled_slices_depth[0], filled_slices_depth[-1]\n",
    "\n",
    "    #Cropping step, iterating for every module in the same example to be cropped.\n",
    "    for module_path in modules_list:\n",
    "        module_vol = nib.load(module_path)\n",
    "        cropped_vol = module_vol.get_fdata()[min_hei_idx : (max_hei_idx+1),\n",
    "                                             min_wid_idx : (max_wid_idx+1),\n",
    "                                             min_dep_idx : (max_dep_idx+1)]\n",
    "\n",
    "        nifti_img =  nib.Nifti1Image(cropped_vol, module_vol.affine)            # to save this 3D (ndarry) numpy\n",
    "\n",
    "        #to make the naming of the resulted folders and files the same as the original naming\n",
    "        vol_new_path = results_path + '/' + data_list[example].split('\\\\')[-1]\n",
    "        if not os.path.exists(vol_new_path):\n",
    "            os.makedirs(vol_new_path)\n",
    "\n",
    "        module_new_path = vol_new_path + '/' + module_path.split('\\\\')[-1]\n",
    "        nib.save(nifti_img, module_new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6dca1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Testing random module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f369d0",
   "metadata": {},
   "source": [
    "##### Non-cropped volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f09f9a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 155)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = r'C:\\Users\\DELL\\Graduation Project\\Slices\\Brats21\\BraTS2021_00000\\BraTS2021_00000_t1.nii.gz'\n",
    "img_test = nib.load(test_path)\n",
    "\n",
    "img_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d5e52",
   "metadata": {},
   "source": [
    "##### brain cropped volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d445476a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 171, 146)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = r'C:\\Users\\DELL\\Graduation Project\\0-100 Niftis Cropped-3D-T Brats\\BraTS2021_00000\\BraTS2021_00000_t1.nii.gz'\n",
    "img_test = nib.load(test_path)\n",
    "\n",
    "img_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ee4d69",
   "metadata": {},
   "source": [
    "##### tumor cropped volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "945c371c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 171, 47)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = r'C:\\Users\\DELL\\Graduation Project\\Datasets\\0-100 Imgs\\0-100 Flair_imgs\\Flair_imgs_middle_only\\BraTS2021_00000\\BraTS2021_00000_t1.nii.gz'\n",
    "img_test = nib.load(test_path)\n",
    "\n",
    "img_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cbabd0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Image shape was (240, 240, 155) Before cropping, now it's (136, 171, 146), and (136, 171, 47) for this example (Random)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
